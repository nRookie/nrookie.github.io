

You could think of the kernel as a server that answers requests; these requests can come either from a process running on a CPU or an external device issuing an interrupt request. We make this analogy to underscore that parts of the kernel are not run serially but in an interleaved way. Thus, they can give rise to race conditions, which must be controlled through proper synchronization techniques. A general introduction to these topics can be found in Section 1.6 in Chapter 1.





We start this chapter by reviewing when, and to what extent, kernel requests are executed in an interleaved fashion. We then introduce four basic synchronization techniques implemented by the kernel and illustrate how they are applied by means of examples.



The next two sections deal with the extension of the Linux kernel to multiprocessor architectures. The first describes some hardware features of the Symmetric Multiprocessor (SMP) architecture, while the second discusses additional mutual exclusion techniques adopted by the SMP version of the Linux kernel.





## 11.1 Kernel Control Paths



As we said, kernel functions are executed following a request that may be issued in two possible ways:

- A process executing in User Mode causes an exception, for instance by executing an int 0x80 assembly language instruction.
- An external device sends a signal to a Programmable Interrupt Controller by using an IRQ line, and the corresponding interrupt is enabled.





The sequence of **instructions executed in Kernel Mode** to handle a kernel request is denoted as **kernel control path** : when a User Mode process issues a system call request, for instance, the first instructions of the corresponding kernel control path are those included in the initial part of the system_call( ) function, while the last instructions are those included in the ret_from_sys_call( ) function.





In the simplest cases, the CPU executes a kernel control path sequentially from the first instruction to the last. When one of the following events occurs, however, the CPU interleaves kernel control paths:



• A context switch occurs. As we have seen in Chapter 10, a context switch can occur only when the schedule( ) function is invoked.



• An interrupt occurs while the CPU is running a kernel control path with interrupts enabled. In this case, the first kernel control path is left unfinished and the CPU starts processing another kernel control path to handle the interrupt.





It is important to interleave kernel control paths in order to implement multiprocessing. In addition, as already noticed in Section 4.3 in Chapter 4, interleaving improves the throughput of programmable interrupt controllers and device controllers.





While interleaving kernel control paths, special care must be applied to data structures that contain several related member variables, for instance, a buffer and an integer indicating its length. All statements affecting such a data structure must be put into a single critical section, otherwise, it is in danger of being corrupted.





## 11.2 Synchronization Techniques



Chapter 1 introduced the concepts of race condition and critical region for processes. The same definitions apply to kernel control paths. In this chapter, a race condition can occur when the outcome of some computation depends on how two or more interleaved kernel control paths are nested. A critical region is any section of code that should be completely executed by each kernel control path that begins it, before another kernel control path can enter it.



We now examine how kernel control paths can be interleaved while avoiding race conditions among shared data. We'll distinguish four broad types of synchronization techniques:



- Nonpreemptability of processes in Kernel Mode
- Atomic operations
- Interrupt disabling
- Locking



### 11.2.1 Nonpreemptability of Processes in Kernel Mode



As already pointed out, the Linux kernel is not preemptive, that is, a running process cannot be preempted (replaced by a higher-priority process) while it remains in Kernel Mode. In particular, the following assertions always hold in Linux:



No process running in Kernel Mode may be replaced by another process, except when the former voluntarily relinquishes control of the CPU.[1]

[1] Of course, all context switches are performed in Kernel Mode. However, a context switch may occur only when the current process is going to return in User Mode.

- Interrupt or exception handling can interrupt a process running in Kernel Mode; however, when the interrupt handler terminates, the kernel control path of the process is resumed.
- A kernel control path performing interrupt or exception handling can be interrupted only by another control path performing interrupt or exception handling.



Thanks to the above assertions, kernel control paths dealing with nonblocking system calls are atomic with respect to other control paths started by system calls. This simplifies the implementation of many kernel functions: any kernel data structures that are not updated by interrupt or exception handlers can be safely accessed. However, if a process in Kernel Mode voluntarily relinquishes the CPU, it must ensure that all data structures are left in a consistent state. Moreover, when it resumes its execution, it must recheck the value of all previously accessed data structures that could be changed. The change could be caused by a different kernel control path, possibly running the same code on behalf of a separate process.



### 11.2.2 Atomic Operations



The easiest way to prevent race conditions is by ensuring that an operation is atomic at the chip level: the operation must be executed in a single instruction. These very small atomic operations can be found at the base of other, more flexible mechanisms to create critical sections.



Thus, an atomic operation is something that can be performed by executing a single assembly language instruction in an "atomic" way, that is, without being interrupted in the middle.



Let's review Intel 80x86 instructions according to that classification:



- Assembly language instructions that make zero or one memory access are atomic.

- Read/modify/write assembly language instructions such as inc or dec that read data from memory, update it, and write the updated value back to memory are atomic if no other processor has taken the memory bus after the read and before the write. Memory bus stealing, naturally, never happens in a uniprocessor system, because all memory

  accesses are made by the same processor.

- Read/modify/write assembly language instructions whose opcode is prefixed by the

  lock byte (0xf0) are atomic even on a multiprocessor system. When the control unit detects the prefix, it "locks" the memory bus until the instruction is finished. Therefore, other processors cannot access the memory location while the locked instruction is being executed.

- Assembly language instructions whose opcode is prefixed by a rep byte (0xf2, 0xf3), which forces the control unit to repeat the same instruction several times, are not atomic: the control unit checks for pending interrupts before executing a new iteration.





When you write C code, you cannot guarantee that the compiler will use a single, atomic instruction for an operation like a=a+1 or even for a++. Thus, the Linux kernel provides special functions (see Table 11-1) that it implements as single, atomic assembly language instructions; on multiprocessor systems each such instruction is prefixed by a lock byte.



![image-20211203115514390](/Users/user/playground/share/nrookie.github.io/collections/linux/kernel/image-20211203115514390.png)





### 11.2.3 Interrupt Disabling



For any section of code too large to be defined as an atomic operation, more complicated means of providing critical sections are needed. To ensure that no window is left open for a race condition to slip in, even a window one instruction long, these critical sections always have an atomic operation at their base.

Interrupt disabling is one of the key mechanisms used to ensure that a sequence of kernel statements is operated as a critical section. It allows a kernel control path to continue executing even when hardware devices issue IRQ signals, thus providing an effective way to protect data structures that are also accessed by interrupt handlers.



However, interrupt disabling alone does not always prevent kernel control path interleaving. Indeed, a kernel control path could raise a "Page fault" exception, which in turn could suspend the current process (and thus the corresponding kernel control path). Or again, a kernel control path could directly invoke the schedule( ) function. This happens during most I/O disk operations because they are potentially blocking, that is, they may force the process to sleep until the I/O operation completes. Therefore, the kernel must never execute a blocking operation when interrupts are disabled, since the system could freeze.



Interrupts can be disabled by means of the cli assembly language instruction, which is yielded by the _ _cli( ) and cli( ) macros. Interrupts can be enabled by means of the sti assembly language instruction, which is yielded by the __sti( ) and sti( ) macros. On a uniprocessor system cli( ) is equivalent to __cli( ) and sti( ) is equivalent to __sti( ); however, as we shall see later in this chapter, these macros are quite different on a multiprocessor system.





When the kernel enters a critical section, it clears the IF flag of the eflags register in order to disable interrupts. But at the end of the critical section, the kernel can't simply set the flag again. Interrupts can execute in nested fashion, so the kernel does not know what the IF flag was before the current control path executed. Each control path must therefore save the old setting of the flag and restore that setting at the end.





In order to save the eflags content, the kernel uses the __save_flags macro; on a uniprocessor system it is identical to the save_flags macro. In order to restore the eflags content, the kernel uses the _ _restore_flags and (on a uniprocessor system) restore_flags macros. Typically, these macros are used in the following way:





``` shell
__save_flags(old);
__cli(  );
[...]
__restore_flags(old);
```



``` shell
The __save_flags macro copies the content of the eflags register into the old local variable; the IF flag is then cleared by __cli( ). At the end of the critical region, the __restore_flags macro restores the original content of eflags; therefore, interrupts are enabled only if they were enabled before this control path issued the __cli( ) macro.
```

 

Linux offers several additional synchronization macros that are important on a multiprocessor system (see Section 11.4.2 later in this chapter) but are somewhat redundant on a uniprocessor system (see Table 11-2). Notice that some functions do not perform any visible operation. They just act as "barriers" for the gcc compiler, since they prevent the compiler from optimizing the code by moving around assembly language instructions. The lck parameter is always ignored.



![image-20211203115940326](/Users/user/playground/share/nrookie.github.io/collections/linux/kernel/image-20211203115940326.png)





Let us recall a few examples of how these macros are used in functions introduced in previous chapters:



- The add_wait_queue( ) and remove_wait_queue( ) functions protect the wait queue list with the write_lock_irqsave( ) and write_unlock_irqrestore( ) functions.
- The setup_x86_irq( ) adds a new interrupt handler for a specific IRQ; the spin_lock_irqsave( ) and spin_unlock_irqrestore( ) functions are used to protect the corresponding list of handlers.
- The run_timer_list( ) function protects the dynamic timer data structures with the spin_lock_irq( ) and spin_unlock_irq( ) functions.
- The handle_signal( ) function protects the blocked field of current with the spin_lock_irq( ) and spin_unlock_irq( ) functions.





Because of its simplicity, interrupt disabling is widely used by kernel functions for implementing critical regions. Clearly, the critical regions obtained by interrupt disabling must be short, because any kind of communication between the I/O device controllers and the CPU is blocked when the kernel enters one. Longer critical regions should be implemented by means of locking.



### 11.2.4 Locking Through Kernel Semaphores









1. A widely used synchronization technique is locking: when a kernel control path must access a shared data structure or enter a critical region, it must acquire a "lock" for it. A resource protected by a locking mechanism is quite similar to a resource confined in a room whose door is locked when someone is inside. If a kernel control path wishes to access the resource, it tries to "open the door" by acquiring the lock. It will succeed only if the resource is free. Then, as long as it wants to use the resource, the door remains locked. When the kernel control path releases the lock, the door is unlocked and another kernel control path may enter the room.





Linux offers two kinds of locking: kernel semaphores, which are widely used both on uniprocessor systems and multiprocessor ones, and spin locks, which are used only on multiprocessors systems. We'll discuss just kernel semaphores here; the other solution will be discussed in the Section 11.4.2 later in this chapter. When a kernel control path tries to acquire a busy resource protected by a kernel semaphore, the corresponding process is suspended. It will become runnable again when the resource is released.





Kernel semaphores are objects of type struct semaphore and have these fields: count

Stores an integer value. If it is greater than 0, the resource is free, that is, it is currently available. Conversely, if count is less than or equal to 0, the semaphore is busy, that is, the protected resource is currently unavailable. In the latter case, the absolute value of count denotes the number of kernel control paths waiting for the resource. Zero means that a kernel control path is using the resource but no other kernel control path is waiting for it.

wait

Stores the address of a wait queue list that includes all sleeping processes that are currently waiting for the resource. Of course, if count is greater than or equal to 0, the wait queue is empty.





waking

Ensures that, when the resource is freed and the sleeping processes is woken up, only one of them succeeds in acquiring the resource. We'll see this field in operation soon.

 









## 11.3 The SMP Architecture





Symmetrical multiprocessing (SMP ) denotes a multiprocessor architecture in which no CPU is selected as the Master CPU, but rather all of them cooperate on an equal basis, hence the name "symmetrical." As usual, we shall focus on Intel SMP architectures.



How many independent CPUs are most profitably included in a multiprocessor system is a hot issue. The troubles are mainly due to the impressive progress reached in the area of cache systems. Many of the benefits introduced by hardware caches are lost by wasting bus cycles in synchronizing the local hardware caches located on the CPU chips. The higher the number of CPUs, the worse the problem becomes.



From the kernel design point of view, however, we can completely ignore this issue: an SMP kernel remains the same no matter how many CPUs are involved. The big jump in complexity occurs when moving from one CPU (a uniprocessor system) to two.



Before proceeding in describing the changes that had to be made to Linux in order to make it a true SMP kernel, we shall briefly review the hardware features of the Pentium dual- processing systems. These features lie in the following areas of computer architecture:





- Shared memory
- Hardware cache synchronization
- Atomic operations
- Distributed interrupt handling
- Interrupt signals for CPU synchronization

Some hardware issues are completely resolved within the hardware, so we don't have to say much about them.





### 11.3.1 Common Memory





All the CPUs share the same memory; that is, they are connected to a common bus. This means that RAM chips may be accessed concurrently by independent CPUs. Since read or write operations on a RAM chip must be performed serially, a hardware circuit called a memory arbiter is inserted between the bus and every RAM chip. Its role is to grant access to a CPU if the chip is free and to delay it if the chip is busy. Even uniprocessor systems make use of memory arbiters, since they include a specialized processor called DMA that operates concurrently with the CPU (see Section 13.1.4, in Chapter 13).



In the case of multiprocessor systems, the structure of the arbiter is more complex since it has more input ports. The dual Pentium, for instance, maintains a two-port arbiter at each chip entrance and requires that the two CPUs exchange synchronization messages before attempting to use the bus. From the programming point of view, the arbiter is hidden since it is managed by hardware circuits.





### 11.3.2 Hardware Support to Cache Synchronization





The section Section 2.4.6 in Chapter 2,explained that the contents of the hardware cache and the RAM maintain their consistency at the hardware level. The same approach holds in the case of a dual processor. As shown in Figure 11-1, each CPU has its own local hardware cache. But now updating becomes more time-consuming: whenever a CPU modifies its hardware cache it must check whether the same data is contained in the other hardware cache and, if so, notify the other CPU to update it with the proper value. This activity is often called **cache snooping**. Luckily, all this is done at the hardware level and is of no concern to the kernel.



![image-20211203135740512](/Users/user/playground/share/nrookie.github.io/collections/linux/kernel/image-20211203135740512.png)



### 11.3.3 SMP Atomic Operations



Atomic operations for uniprocessor systems have already been introduced in Section 11.2.2. Since standard read-modify-write instructions actually access the memory bus twice, they are not atomic on a multiprocessor system.



Let us give a simple example of what might happen if an SMP kernel used standard instructions. Consider the semaphore implementation described in Section 11.2.4 earlier in this chapter and assume that the down( ) function decrements and tests the count field of the semaphore with a simple decl assembly language instruction. What happens if two processes running on two different CPUs simultaneously execute the decl instruction on the same semaphore? Well, decl is a read-modify-write instruction that accesses the same memory location twice: once to read the old value and again to write the new value.











### 11.4.1 Main SMP Data Structures





In order to handle several CPUs, the kernel must be able to represent the activity that takes place on each of them. In this section we'll consider some significant kernel data structures that have been added to allow multiprocessing.

The most important information is what process is currently running on each CPU, but this information actually does not require a new CPU-specific data structure. Instead, each CPU retrieves the current process through the same current macro defined for uniprocessor systems: since it extracts the process descriptor address from the esp stack pointer register, it yields a value that is CPU-dependent.

A first group of new CPU-specific variables refers to the SMP architecture. Linux/SMP has a hard-wired limit on the number of CPUs, which is defined by the NR_CPUS macro (usually 32).



During the initialization phase, Linux running on the booting CPU probes whether other CPUs exist (some CPU slots of an SMP board may be empty). As a result, both a counter and a bitmap are initialized: max_cpus stores the number of existing CPUs while cpu_present_map specifies which slots contain a CPU.





An existing CPU is not necessarily activated, that is, initialized and recognized by the kernel. Another pair of variables, a counter called smp_num_cpus and a bitmap called cpu_online_map, keeps track of the activated CPUs. If some CPU cannot be properly initialized, the kernel clears the corresponding bit in cpu_online_map.



Each active CPU is identified in Linux by a sequential logical number called CPU ID, which does not necessarily coincide with the CPU slot number. The cpu_number_map and _ _cpu_logical_map arrays allow conversion between CPU IDs and CPU slot numbers.



The process descriptor includes the following fields representing the relationships between the process and a processor:







### has_cpu

​	Flag denoting whether the process is currently running (value 1) or not running (value 0)



### processor

Logical number of the CPU that is running the process, or NO_PROC_ID if the process is not running.



The smp_processor_id( ) macro returns the value of current->processor, that is, the logical number of the CPU that executes the process.





When a new process is created by fork( ), the has_cpu and processor fields of its descriptor are initialized respectively to and to the value NO_PROC_ID. When the schedule( ) function selects a new process to run, it sets its has_cpu field to 1 and its processor field to the logical number of the CPU that is doing the task switch. The corresponding fields of the process being replaced are set to and to NO_PROC_ID, respectively.





During system initialization smp_num_cpus different swapper processes are created. Each of them has a PID equal to and is bound to a specific CPU. As usual, a swapper process is executed only when the corresponding CPU is idle.





### 11.4.2 Spin Locks



Spin locks are a locking mechanism designed to work in a multiprocessing environment. They are similar to the kernel semaphores described earlier, except that when a process finds the lock closed by another process, it "spins" around repeatedly, executing a tight instruction loop.



Of course, spin locks would be useless in a uniprocessor environment, since the waiting process would keep running, and therefore the process that is holding the lock would not have any chance to release it. In a multiprocessing environment, however, spin locks are much more convenient, since their overhead is very small. In other words, a context switch takes a significant amount of time, so it is more efficient for each process to keep its own CPU and simply spin while waiting for a resource.



Each spin lock is represented by a spinlock_t structure consisting of a single lock field; the values and 1 correspond, respectively, to the "unlocked" and the "locked" state. The SPIN_LOCK_UNLOCKED macro initializes a spin lock to 0.



The functions that operate on spin locks are based on atomic read/modify/write operations; this ensures that the spin lock will be properly updated by a process running on a CPU even if other processes running on different CPUs attempt to modify the spin lock at the same time.



The spin_lock macro is used to acquire a spin lock. It takes the address slp of the spin lock as its parameter and yields essentially the following code:



```
1: lock; btsl $0, slp
   jnc  3f
2: testb $1,slp
   jne 2b
   jmp 1b 
3:
```

 



The btsl atomic instruction copies into the carry flag the value of bit in *slp, then sets the bit. A test is then performed on the carry flag: if it is null, it means that the spin lock was unlocked and hence normal execution continues at label 3 (the f suffix denotes the fact that the label is a "forward" one: it appear in a later line of the program). Otherwise, the tight loop at label 2 (the b suffix denotes a "backward" label) is executed until the spin lock assumes the value 0. Then execution restarts from label 1, since it would be unsafe to proceed without checking whether another processor has grabbed the lock.[4]



The spin_unlock macro releases a previously acquired spin lock; it essentially yields the following code:



```
lock; btrl $0, slp
```





The btrl atomic assembly language instruction clears the bit of the spin lock *slp.



Several other macros have been introduced to handle spin locks; their definitions on a multiprocessor system are described in Table 11-3 (see Table 11-2 for their definitions on a uniprocessor system).



### 11.4.3 Read/Write Spin Locks



Read/write spin locks have been introduced to increase the amount of concurrency inside the kernel. They allow several kernel control paths to simultaneously read the same data structure, as long as no kernel control path modifies it. If a kernel control path wishes to write to the structure, it must acquire the write version of the read/write lock, which grants exclusive access to the resource. Of course, allowing concurrent reads on data structures improves system performance.



Figure 11-4 illustrates two critical regions, C1 and C2, protected by read/write locks. Kernel control paths R0 and R1 are reading the data structures in C1 at the same time, while W0 is waiting to acquire the lock for writing. Kernel control path W1 is writing the data structures in C2, while both R2 and W2 are waiting to acquire the lock for reading and writing, respectively.





Each read/write spin lock is a rwlock_t structure; its lock field is a 32-bit counter that represents the number of kernel control paths currently reading the protected data structure. The highest-order bit of the lock field is the write lock: it is set when a kernel control path is modifying the data structure.[5] The RW_LOCK_UNLOCKED macro initializes the lock field of a read/write spin lock to 0. The read_lock macro, applied to the address rwlp of a read/write spin lock, essentially yields the following code:





